{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conala-tokenized-normal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahafp/Conala-Challenge/blob/master/conala_tokenized_normal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieK0oXM5CwsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import re\n",
        "import torch\n",
        "import numpy\n",
        "from torch import autograd, nn, optim\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import collections\n",
        "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fZ8dG5kC4qX",
        "colab_type": "code",
        "outputId": "9b29b01d-bf60-4b28-b647-9ed29cfe3e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns_yiAEGC7GM",
        "colab_type": "code",
        "outputId": "c727ec3e-40a7-4d59-8f81-0b7a19e5c1b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eUrGetvHVUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "MAX_LENGTH = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STKQU7xMC9a3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VocabIntent :\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n",
        "        self.n_words = 3  # Count SOS and EOS and UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQP-ykdwDF_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VocabCode:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n",
        "        self.n_words = 3  # Count SOS and EOS and UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in re.split('([^a-zA-Z0-9 ])',sentence):\n",
        "          if word is not '':\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zme_042tDJDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def orginize_data(data_type):\n",
        "  json_data = '/content/drive/My Drive/conala/' + data_type  \n",
        "  path = open(json_data, \"r\")\n",
        "  data = json.load(path)\n",
        "  pairs=[]\n",
        "  for dic in data:\n",
        "      if dic[\"rewritten_intent\"] is None:\n",
        "          continue\n",
        "      pairs.append([dic[\"rewritten_intent\"], dic[\"snippet\"]])\n",
        "  return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW5f2MoQDLg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(vocab1_name, vocab2_name, data_type):\n",
        "  pairs=orginize_data(data_type=data_type)\n",
        "  intent_vocab=VocabIntent(vocab1_name)\n",
        "  code_vocab=VocabCode(vocab2_name)\n",
        "\n",
        "  for pair in pairs:\n",
        "    intent_vocab.addSentence(pair[0])\n",
        "    code_vocab.addSentence(pair[1])\n",
        "  \n",
        "  return intent_vocab, code_vocab, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR6w2QLIDPwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_lang, output_lang, pairs= prepare_data('intent', 'code', 'conala-train.json')\n",
        "test_pairs=orginize_data('conala-test.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjYTuOnfF-F6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeS38kFuGJmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5DPvrokGQrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] if word in lang.word2index else UNK_token for word in sentence.split(' ')]\n",
        "\n",
        "def c_indexesFromSentence(lang, sentence):\n",
        "    sen = re.split('([^a-zA-Z0-9 ])',sentence)\n",
        "    return [lang.word2index[word] for word in sen if word is not '']\n",
        "\n",
        "def tensorFromSentence(lang, sentence, key):\n",
        "    if key is 'intent':\n",
        "      indexes = indexesFromSentence(lang, sentence)\n",
        "      indexes.append(EOS_token)\n",
        "    else:\n",
        "      indexes = c_indexesFromSentence(lang, sentence)\n",
        "      indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0], 'intent')\n",
        "    target_tensor =tensorFromSentence(output_lang, pair[1], 'code')\n",
        "    return (input_tensor, target_tensor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBvQ9BydGVQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeLIxUwAGaYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRnWSrgZGd6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    return plot_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLwVGq_AGgmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvhLCGAIGkSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence, 'intent')\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niS62EYRGrMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ''.join(output_words)\n",
        "        output_sentence = output_sentence.strip('<EOS>')\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znXhBsnRVmco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_data_set(encoder, decoder, pairs, n=10):\n",
        "  answer_list=[]\n",
        "  for i, pair in enumerate(pairs,0):\n",
        "      output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "      output_sentence = ''.join(output_words)\n",
        "      answer_list.append(output_sentence.strip('<EOS>'))\n",
        "      if i%20==0:\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "  return answer_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-uwZYvZGuag",
        "colab_type": "code",
        "outputId": "190d66c0-9e25-463f-89d0-50052e7d9697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hidden_size = 300\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.3).to(device)\n",
        "\n",
        "train_losses=trainIters(encoder1, attn_decoder1, 75000, print_every=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 14s (- 177m 14s) (100 0%) 5.8833\n",
            "0m 19s (- 120m 58s) (200 0%) 4.4249\n",
            "0m 23s (- 99m 34s) (300 0%) 4.0122\n",
            "0m 29s (- 91m 1s) (400 0%) 3.9572\n",
            "0m 34s (- 85m 31s) (500 0%) 4.0142\n",
            "0m 39s (- 81m 40s) (600 0%) 3.7121\n",
            "0m 45s (- 79m 49s) (700 0%) 3.9077\n",
            "0m 50s (- 78m 24s) (800 1%) 3.9659\n",
            "0m 56s (- 76m 55s) (900 1%) 3.6768\n",
            "1m 1s (- 75m 23s) (1000 1%) 3.7950\n",
            "1m 6s (- 74m 37s) (1100 1%) 3.6522\n",
            "1m 11s (- 72m 57s) (1200 1%) 3.3156\n",
            "1m 16s (- 71m 59s) (1300 1%) 3.5739\n",
            "1m 21s (- 71m 5s) (1400 1%) 3.6699\n",
            "1m 26s (- 70m 28s) (1500 2%) 3.5587\n",
            "1m 31s (- 70m 3s) (1600 2%) 3.5780\n",
            "1m 36s (- 69m 25s) (1700 2%) 3.5421\n",
            "1m 41s (- 68m 54s) (1800 2%) 3.6031\n",
            "1m 46s (- 68m 9s) (1900 2%) 3.3772\n",
            "1m 51s (- 67m 38s) (2000 2%) 3.4636\n",
            "1m 56s (- 67m 24s) (2100 2%) 3.4545\n",
            "2m 1s (- 66m 52s) (2200 2%) 3.1964\n",
            "2m 6s (- 66m 24s) (2300 3%) 3.4571\n",
            "2m 11s (- 66m 14s) (2400 3%) 3.6411\n",
            "2m 16s (- 65m 59s) (2500 3%) 3.2390\n",
            "2m 21s (- 65m 41s) (2600 3%) 3.2780\n",
            "2m 26s (- 65m 34s) (2700 3%) 3.3851\n",
            "2m 32s (- 65m 23s) (2800 3%) 3.2238\n",
            "2m 37s (- 65m 14s) (2900 3%) 3.2975\n",
            "2m 42s (- 65m 3s) (3000 4%) 3.4094\n",
            "2m 47s (- 64m 54s) (3100 4%) 3.3005\n",
            "2m 53s (- 64m 45s) (3200 4%) 3.1759\n",
            "2m 57s (- 64m 24s) (3300 4%) 3.3467\n",
            "3m 2s (- 64m 7s) (3400 4%) 3.2657\n",
            "3m 7s (- 64m 0s) (3500 4%) 3.3039\n",
            "3m 13s (- 63m 51s) (3600 4%) 3.2753\n",
            "3m 18s (- 63m 44s) (3700 4%) 3.2750\n",
            "3m 23s (- 63m 37s) (3800 5%) 3.3336\n",
            "3m 29s (- 63m 32s) (3900 5%) 3.1688\n",
            "3m 34s (- 63m 25s) (4000 5%) 3.2664\n",
            "3m 39s (- 63m 14s) (4100 5%) 3.2155\n",
            "3m 44s (- 63m 2s) (4200 5%) 3.1814\n",
            "3m 49s (- 62m 59s) (4300 5%) 3.1133\n",
            "3m 54s (- 62m 48s) (4400 5%) 3.1657\n",
            "4m 0s (- 62m 40s) (4500 6%) 3.1455\n",
            "4m 5s (- 62m 37s) (4600 6%) 2.9736\n",
            "4m 10s (- 62m 31s) (4700 6%) 3.0663\n",
            "4m 15s (- 62m 23s) (4800 6%) 3.1084\n",
            "4m 20s (- 62m 13s) (4900 6%) 3.0497\n",
            "4m 26s (- 62m 4s) (5000 6%) 2.9210\n",
            "4m 30s (- 61m 51s) (5100 6%) 2.8983\n",
            "4m 35s (- 61m 42s) (5200 6%) 2.9172\n",
            "4m 40s (- 61m 32s) (5300 7%) 3.0297\n",
            "4m 45s (- 61m 25s) (5400 7%) 3.0878\n",
            "4m 50s (- 61m 15s) (5500 7%) 2.9661\n",
            "4m 56s (- 61m 12s) (5600 7%) 3.1111\n",
            "5m 1s (- 61m 7s) (5700 7%) 3.0272\n",
            "5m 6s (- 61m 1s) (5800 7%) 3.1567\n",
            "5m 12s (- 60m 55s) (5900 7%) 3.0632\n",
            "5m 17s (- 60m 56s) (6000 8%) 3.0460\n",
            "5m 23s (- 60m 51s) (6100 8%) 2.9563\n",
            "5m 28s (- 60m 45s) (6200 8%) 2.9579\n",
            "5m 34s (- 60m 43s) (6300 8%) 3.0390\n",
            "5m 39s (- 60m 35s) (6400 8%) 2.9332\n",
            "5m 44s (- 60m 28s) (6500 8%) 2.9953\n",
            "5m 49s (- 60m 23s) (6600 8%) 2.9311\n",
            "5m 54s (- 60m 12s) (6700 8%) 2.7831\n",
            "5m 59s (- 60m 4s) (6800 9%) 2.7942\n",
            "6m 5s (- 60m 3s) (6900 9%) 2.9448\n",
            "6m 10s (- 59m 57s) (7000 9%) 2.7967\n",
            "6m 15s (- 59m 52s) (7100 9%) 2.9701\n",
            "6m 20s (- 59m 46s) (7200 9%) 3.0028\n",
            "6m 26s (- 59m 41s) (7300 9%) 2.8162\n",
            "6m 31s (- 59m 34s) (7400 9%) 2.7067\n",
            "6m 36s (- 59m 28s) (7500 10%) 2.6758\n",
            "6m 41s (- 59m 23s) (7600 10%) 2.7589\n",
            "6m 47s (- 59m 17s) (7700 10%) 2.7635\n",
            "6m 52s (- 59m 12s) (7800 10%) 2.8059\n",
            "6m 57s (- 59m 4s) (7900 10%) 2.6244\n",
            "7m 2s (- 59m 2s) (8000 10%) 2.7504\n",
            "7m 7s (- 58m 54s) (8100 10%) 2.6017\n",
            "7m 13s (- 58m 47s) (8200 10%) 2.8819\n",
            "7m 18s (- 58m 41s) (8300 11%) 2.6037\n",
            "7m 23s (- 58m 34s) (8400 11%) 2.5675\n",
            "7m 28s (- 58m 28s) (8500 11%) 2.7194\n",
            "7m 33s (- 58m 22s) (8600 11%) 2.5048\n",
            "7m 38s (- 58m 14s) (8700 11%) 2.6741\n",
            "7m 44s (- 58m 11s) (8800 11%) 2.6602\n",
            "7m 49s (- 58m 7s) (8900 11%) 2.8015\n",
            "7m 54s (- 58m 1s) (9000 12%) 2.5721\n",
            "7m 59s (- 57m 53s) (9100 12%) 2.5250\n",
            "8m 5s (- 57m 50s) (9200 12%) 2.6134\n",
            "8m 10s (- 57m 45s) (9300 12%) 2.5939\n",
            "8m 15s (- 57m 40s) (9400 12%) 2.4909\n",
            "8m 20s (- 57m 34s) (9500 12%) 2.6204\n",
            "8m 25s (- 57m 26s) (9600 12%) 2.7059\n",
            "8m 31s (- 57m 21s) (9700 12%) 2.4342\n",
            "8m 36s (- 57m 15s) (9800 13%) 2.3903\n",
            "8m 41s (- 57m 9s) (9900 13%) 2.6761\n",
            "8m 46s (- 57m 2s) (10000 13%) 2.4534\n",
            "8m 51s (- 56m 56s) (10100 13%) 2.4526\n",
            "8m 56s (- 56m 51s) (10200 13%) 2.4012\n",
            "9m 2s (- 56m 45s) (10300 13%) 2.5785\n",
            "9m 7s (- 56m 38s) (10400 13%) 2.2925\n",
            "9m 12s (- 56m 33s) (10500 14%) 2.5705\n",
            "9m 17s (- 56m 26s) (10600 14%) 2.6519\n",
            "9m 22s (- 56m 21s) (10700 14%) 2.5579\n",
            "9m 28s (- 56m 17s) (10800 14%) 2.3046\n",
            "9m 33s (- 56m 11s) (10900 14%) 2.5766\n",
            "9m 38s (- 56m 4s) (11000 14%) 2.5161\n",
            "9m 43s (- 55m 58s) (11100 14%) 2.4746\n",
            "9m 48s (- 55m 52s) (11200 14%) 2.3908\n",
            "9m 54s (- 55m 48s) (11300 15%) 2.4259\n",
            "9m 58s (- 55m 41s) (11400 15%) 2.4336\n",
            "10m 4s (- 55m 35s) (11500 15%) 2.3800\n",
            "10m 9s (- 55m 29s) (11600 15%) 2.6091\n",
            "10m 13s (- 55m 21s) (11700 15%) 2.3539\n",
            "10m 19s (- 55m 16s) (11800 15%) 2.5212\n",
            "10m 24s (- 55m 10s) (11900 15%) 2.3945\n",
            "10m 29s (- 55m 3s) (12000 16%) 2.2659\n",
            "10m 34s (- 54m 58s) (12100 16%) 2.1858\n",
            "10m 39s (- 54m 53s) (12200 16%) 2.3649\n",
            "10m 45s (- 54m 49s) (12300 16%) 2.4605\n",
            "10m 50s (- 54m 44s) (12400 16%) 2.5039\n",
            "10m 56s (- 54m 40s) (12500 16%) 2.3061\n",
            "11m 1s (- 54m 34s) (12600 16%) 2.2861\n",
            "11m 6s (- 54m 29s) (12700 16%) 2.3952\n",
            "11m 12s (- 54m 25s) (12800 17%) 2.4865\n",
            "11m 16s (- 54m 18s) (12900 17%) 2.3779\n",
            "11m 22s (- 54m 13s) (13000 17%) 2.3462\n",
            "11m 27s (- 54m 9s) (13100 17%) 2.5488\n",
            "11m 32s (- 54m 3s) (13200 17%) 2.4334\n",
            "11m 38s (- 54m 0s) (13300 17%) 2.5502\n",
            "11m 43s (- 53m 54s) (13400 17%) 2.4319\n",
            "11m 48s (- 53m 48s) (13500 18%) 2.4483\n",
            "11m 54s (- 53m 43s) (13600 18%) 2.2821\n",
            "11m 59s (- 53m 37s) (13700 18%) 2.3407\n",
            "12m 4s (- 53m 32s) (13800 18%) 2.1554\n",
            "12m 10s (- 53m 29s) (13900 18%) 2.4431\n",
            "12m 15s (- 53m 23s) (14000 18%) 2.2233\n",
            "12m 20s (- 53m 17s) (14100 18%) 2.0993\n",
            "12m 25s (- 53m 14s) (14200 18%) 2.4262\n",
            "12m 31s (- 53m 9s) (14300 19%) 2.1288\n",
            "12m 37s (- 53m 6s) (14400 19%) 2.1072\n",
            "12m 43s (- 53m 4s) (14500 19%) 2.4935\n",
            "12m 48s (- 52m 59s) (14600 19%) 2.2373\n",
            "12m 53s (- 52m 54s) (14700 19%) 2.2137\n",
            "12m 58s (- 52m 48s) (14800 19%) 2.2810\n",
            "13m 4s (- 52m 44s) (14900 19%) 2.3503\n",
            "13m 9s (- 52m 39s) (15000 20%) 2.2201\n",
            "13m 15s (- 52m 34s) (15100 20%) 2.0644\n",
            "13m 20s (- 52m 28s) (15200 20%) 2.1342\n",
            "13m 25s (- 52m 22s) (15300 20%) 2.0293\n",
            "13m 30s (- 52m 17s) (15400 20%) 2.1793\n",
            "13m 35s (- 52m 12s) (15500 20%) 2.2866\n",
            "13m 40s (- 52m 5s) (15600 20%) 2.2732\n",
            "13m 46s (- 52m 0s) (15700 20%) 2.1349\n",
            "13m 51s (- 51m 55s) (15800 21%) 2.1711\n",
            "13m 57s (- 51m 52s) (15900 21%) 2.3392\n",
            "14m 2s (- 51m 46s) (16000 21%) 2.3884\n",
            "14m 7s (- 51m 41s) (16100 21%) 2.1247\n",
            "14m 12s (- 51m 35s) (16200 21%) 1.8864\n",
            "14m 18s (- 51m 29s) (16300 21%) 2.0697\n",
            "14m 23s (- 51m 23s) (16400 21%) 2.0253\n",
            "14m 28s (- 51m 17s) (16500 22%) 1.8597\n",
            "14m 33s (- 51m 14s) (16600 22%) 2.0068\n",
            "14m 38s (- 51m 8s) (16700 22%) 1.8147\n",
            "14m 44s (- 51m 3s) (16800 22%) 2.1103\n",
            "14m 49s (- 50m 59s) (16900 22%) 2.1664\n",
            "14m 55s (- 50m 55s) (17000 22%) 2.0494\n",
            "15m 0s (- 50m 48s) (17100 22%) 1.8108\n",
            "15m 5s (- 50m 43s) (17200 22%) 2.1374\n",
            "15m 11s (- 50m 38s) (17300 23%) 2.1148\n",
            "15m 16s (- 50m 33s) (17400 23%) 2.0489\n",
            "15m 21s (- 50m 26s) (17500 23%) 1.8877\n",
            "15m 26s (- 50m 20s) (17600 23%) 2.0177\n",
            "15m 31s (- 50m 15s) (17700 23%) 2.1071\n",
            "15m 36s (- 50m 9s) (17800 23%) 2.0512\n",
            "15m 41s (- 50m 3s) (17900 23%) 2.0264\n",
            "15m 46s (- 49m 57s) (18000 24%) 2.0134\n",
            "15m 52s (- 49m 53s) (18100 24%) 2.0681\n",
            "15m 57s (- 49m 49s) (18200 24%) 2.3066\n",
            "16m 3s (- 49m 44s) (18300 24%) 2.0985\n",
            "16m 9s (- 49m 40s) (18400 24%) 2.0927\n",
            "16m 14s (- 49m 35s) (18500 24%) 1.9101\n",
            "16m 19s (- 49m 29s) (18600 24%) 1.8767\n",
            "16m 24s (- 49m 23s) (18700 24%) 1.8463\n",
            "16m 29s (- 49m 18s) (18800 25%) 2.0831\n",
            "16m 35s (- 49m 13s) (18900 25%) 1.8237\n",
            "16m 40s (- 49m 8s) (19000 25%) 2.0675\n",
            "16m 45s (- 49m 3s) (19100 25%) 1.8993\n",
            "16m 51s (- 48m 58s) (19200 25%) 2.1602\n",
            "16m 56s (- 48m 54s) (19300 25%) 2.0155\n",
            "17m 2s (- 48m 49s) (19400 25%) 2.0182\n",
            "17m 8s (- 48m 45s) (19500 26%) 2.0094\n",
            "17m 13s (- 48m 40s) (19600 26%) 1.9512\n",
            "17m 18s (- 48m 34s) (19700 26%) 2.0054\n",
            "17m 23s (- 48m 28s) (19800 26%) 1.8590\n",
            "17m 28s (- 48m 23s) (19900 26%) 2.1049\n",
            "17m 34s (- 48m 18s) (20000 26%) 2.0227\n",
            "17m 39s (- 48m 13s) (20100 26%) 1.8994\n",
            "17m 44s (- 48m 7s) (20200 26%) 2.1516\n",
            "17m 50s (- 48m 3s) (20300 27%) 1.9833\n",
            "17m 55s (- 47m 58s) (20400 27%) 1.8655\n",
            "18m 1s (- 47m 54s) (20500 27%) 1.9370\n",
            "18m 6s (- 47m 49s) (20600 27%) 1.8962\n",
            "18m 12s (- 47m 45s) (20700 27%) 2.0596\n",
            "18m 18s (- 47m 41s) (20800 27%) 1.9579\n",
            "18m 23s (- 47m 36s) (20900 27%) 2.1875\n",
            "18m 28s (- 47m 31s) (21000 28%) 1.9577\n",
            "18m 34s (- 47m 26s) (21100 28%) 1.7174\n",
            "18m 39s (- 47m 21s) (21200 28%) 1.9200\n",
            "18m 45s (- 47m 16s) (21300 28%) 1.9646\n",
            "18m 51s (- 47m 13s) (21400 28%) 2.0150\n",
            "18m 56s (- 47m 8s) (21500 28%) 1.9832\n",
            "19m 2s (- 47m 4s) (21600 28%) 2.0636\n",
            "19m 7s (- 46m 59s) (21700 28%) 1.9328\n",
            "19m 13s (- 46m 54s) (21800 29%) 1.8861\n",
            "19m 18s (- 46m 48s) (21900 29%) 1.9888\n",
            "19m 24s (- 46m 44s) (22000 29%) 2.0137\n",
            "19m 29s (- 46m 39s) (22100 29%) 2.1217\n",
            "19m 34s (- 46m 34s) (22200 29%) 2.0369\n",
            "19m 39s (- 46m 28s) (22300 29%) 1.8386\n",
            "19m 45s (- 46m 24s) (22400 29%) 2.0814\n",
            "19m 51s (- 46m 19s) (22500 30%) 1.9642\n",
            "19m 56s (- 46m 14s) (22600 30%) 1.7193\n",
            "20m 2s (- 46m 9s) (22700 30%) 2.0243\n",
            "20m 7s (- 46m 4s) (22800 30%) 1.8622\n",
            "20m 12s (- 45m 59s) (22900 30%) 2.0182\n",
            "20m 18s (- 45m 54s) (23000 30%) 1.8728\n",
            "20m 23s (- 45m 48s) (23100 30%) 1.7388\n",
            "20m 28s (- 45m 43s) (23200 30%) 1.9444\n",
            "20m 34s (- 45m 38s) (23300 31%) 1.7391\n",
            "20m 39s (- 45m 33s) (23400 31%) 1.7854\n",
            "20m 44s (- 45m 27s) (23500 31%) 2.0064\n",
            "20m 50s (- 45m 23s) (23600 31%) 1.8474\n",
            "20m 55s (- 45m 17s) (23700 31%) 1.7910\n",
            "21m 1s (- 45m 12s) (23800 31%) 1.9022\n",
            "21m 6s (- 45m 7s) (23900 31%) 1.7949\n",
            "21m 11s (- 45m 1s) (24000 32%) 1.9026\n",
            "21m 16s (- 44m 56s) (24100 32%) 1.8614\n",
            "21m 21s (- 44m 50s) (24200 32%) 1.9450\n",
            "21m 27s (- 44m 45s) (24300 32%) 1.9660\n",
            "21m 32s (- 44m 41s) (24400 32%) 2.0153\n",
            "21m 37s (- 44m 35s) (24500 32%) 1.8073\n",
            "21m 43s (- 44m 29s) (24600 32%) 1.7340\n",
            "21m 48s (- 44m 24s) (24700 32%) 1.8002\n",
            "21m 53s (- 44m 18s) (24800 33%) 1.8172\n",
            "21m 58s (- 44m 12s) (24900 33%) 1.7705\n",
            "22m 3s (- 44m 7s) (25000 33%) 1.6893\n",
            "22m 8s (- 44m 1s) (25100 33%) 1.9352\n",
            "22m 14s (- 43m 56s) (25200 33%) 1.7053\n",
            "22m 19s (- 43m 50s) (25300 33%) 1.9037\n",
            "22m 24s (- 43m 45s) (25400 33%) 1.9749\n",
            "22m 29s (- 43m 39s) (25500 34%) 1.6797\n",
            "22m 35s (- 43m 35s) (25600 34%) 1.8309\n",
            "22m 40s (- 43m 29s) (25700 34%) 1.7497\n",
            "22m 45s (- 43m 24s) (25800 34%) 1.7195\n",
            "22m 51s (- 43m 19s) (25900 34%) 1.7812\n",
            "22m 56s (- 43m 13s) (26000 34%) 1.9091\n",
            "23m 1s (- 43m 8s) (26100 34%) 1.7279\n",
            "23m 6s (- 43m 3s) (26200 34%) 1.6242\n",
            "23m 12s (- 42m 57s) (26300 35%) 1.7255\n",
            "23m 17s (- 42m 52s) (26400 35%) 1.8633\n",
            "23m 22s (- 42m 47s) (26500 35%) 1.6970\n",
            "23m 28s (- 42m 42s) (26600 35%) 1.7646\n",
            "23m 33s (- 42m 36s) (26700 35%) 1.5747\n",
            "23m 38s (- 42m 31s) (26800 35%) 1.6241\n",
            "23m 44s (- 42m 26s) (26900 35%) 1.8424\n",
            "23m 49s (- 42m 21s) (27000 36%) 1.8820\n",
            "23m 54s (- 42m 16s) (27100 36%) 1.6425\n",
            "24m 0s (- 42m 11s) (27200 36%) 1.6686\n",
            "24m 5s (- 42m 6s) (27300 36%) 1.6875\n",
            "24m 11s (- 42m 1s) (27400 36%) 1.7186\n",
            "24m 16s (- 41m 56s) (27500 36%) 1.5119\n",
            "24m 22s (- 41m 50s) (27600 36%) 1.7810\n",
            "24m 27s (- 41m 45s) (27700 36%) 1.6552\n",
            "24m 33s (- 41m 40s) (27800 37%) 1.7060\n",
            "24m 38s (- 41m 35s) (27900 37%) 1.5579\n",
            "24m 44s (- 41m 31s) (28000 37%) 1.7377\n",
            "24m 49s (- 41m 25s) (28100 37%) 1.6464\n",
            "24m 54s (- 41m 19s) (28200 37%) 1.7235\n",
            "24m 59s (- 41m 13s) (28300 37%) 1.5854\n",
            "25m 4s (- 41m 8s) (28400 37%) 1.5519\n",
            "25m 9s (- 41m 3s) (28500 38%) 1.6820\n",
            "25m 15s (- 40m 58s) (28600 38%) 1.8754\n",
            "25m 20s (- 40m 52s) (28700 38%) 1.6443\n",
            "25m 25s (- 40m 47s) (28800 38%) 1.5696\n",
            "25m 30s (- 40m 41s) (28900 38%) 1.6380\n",
            "25m 36s (- 40m 36s) (29000 38%) 1.5273\n",
            "25m 40s (- 40m 30s) (29100 38%) 1.6636\n",
            "25m 46s (- 40m 24s) (29200 38%) 1.6974\n",
            "25m 51s (- 40m 19s) (29300 39%) 1.7125\n",
            "25m 56s (- 40m 14s) (29400 39%) 1.5553\n",
            "26m 2s (- 40m 9s) (29500 39%) 1.8042\n",
            "26m 7s (- 40m 4s) (29600 39%) 1.8717\n",
            "26m 12s (- 39m 59s) (29700 39%) 1.7571\n",
            "26m 18s (- 39m 54s) (29800 39%) 1.6238\n",
            "26m 23s (- 39m 49s) (29900 39%) 1.5405\n",
            "26m 29s (- 39m 43s) (30000 40%) 1.6415\n",
            "26m 34s (- 39m 38s) (30100 40%) 1.5216\n",
            "26m 40s (- 39m 34s) (30200 40%) 1.5310\n",
            "26m 45s (- 39m 29s) (30300 40%) 1.6617\n",
            "26m 51s (- 39m 24s) (30400 40%) 1.3779\n",
            "26m 56s (- 39m 18s) (30500 40%) 1.6410\n",
            "27m 2s (- 39m 13s) (30600 40%) 1.8067\n",
            "27m 7s (- 39m 8s) (30700 40%) 1.6514\n",
            "27m 13s (- 39m 3s) (30800 41%) 1.7320\n",
            "27m 18s (- 38m 58s) (30900 41%) 1.7371\n",
            "27m 23s (- 38m 52s) (31000 41%) 1.6013\n",
            "27m 28s (- 38m 47s) (31100 41%) 1.6879\n",
            "27m 34s (- 38m 42s) (31200 41%) 1.5866\n",
            "27m 40s (- 38m 37s) (31300 41%) 1.5909\n",
            "27m 45s (- 38m 32s) (31400 41%) 1.6408\n",
            "27m 50s (- 38m 27s) (31500 42%) 1.6762\n",
            "27m 56s (- 38m 22s) (31600 42%) 1.7660\n",
            "28m 1s (- 38m 16s) (31700 42%) 1.6808\n",
            "28m 6s (- 38m 11s) (31800 42%) 1.5675\n",
            "28m 11s (- 38m 5s) (31900 42%) 1.3896\n",
            "28m 16s (- 38m 0s) (32000 42%) 1.6466\n",
            "28m 22s (- 37m 55s) (32100 42%) 1.6291\n",
            "28m 28s (- 37m 50s) (32200 42%) 1.7838\n",
            "28m 33s (- 37m 45s) (32300 43%) 1.7360\n",
            "28m 38s (- 37m 39s) (32400 43%) 1.5188\n",
            "28m 43s (- 37m 33s) (32500 43%) 1.5187\n",
            "28m 48s (- 37m 28s) (32600 43%) 1.4847\n",
            "28m 54s (- 37m 23s) (32700 43%) 1.7055\n",
            "28m 59s (- 37m 17s) (32800 43%) 1.6359\n",
            "29m 4s (- 37m 12s) (32900 43%) 1.5717\n",
            "29m 10s (- 37m 7s) (33000 44%) 1.5599\n",
            "29m 15s (- 37m 1s) (33100 44%) 1.5426\n",
            "29m 20s (- 36m 56s) (33200 44%) 1.5887\n",
            "29m 26s (- 36m 51s) (33300 44%) 1.6924\n",
            "29m 31s (- 36m 46s) (33400 44%) 1.8027\n",
            "29m 36s (- 36m 40s) (33500 44%) 1.5385\n",
            "29m 41s (- 36m 35s) (33600 44%) 1.5886\n",
            "29m 46s (- 36m 29s) (33700 44%) 1.7208\n",
            "29m 52s (- 36m 24s) (33800 45%) 1.4965\n",
            "29m 57s (- 36m 19s) (33900 45%) 1.5967\n",
            "30m 2s (- 36m 14s) (34000 45%) 1.4811\n",
            "30m 8s (- 36m 8s) (34100 45%) 1.4752\n",
            "30m 13s (- 36m 3s) (34200 45%) 1.4905\n",
            "30m 18s (- 35m 58s) (34300 45%) 1.5562\n",
            "30m 23s (- 35m 52s) (34400 45%) 1.6345\n",
            "30m 29s (- 35m 47s) (34500 46%) 1.6346\n",
            "30m 35s (- 35m 42s) (34600 46%) 1.8145\n",
            "30m 40s (- 35m 37s) (34700 46%) 1.6617\n",
            "30m 46s (- 35m 32s) (34800 46%) 1.5917\n",
            "30m 51s (- 35m 27s) (34900 46%) 1.6358\n",
            "30m 57s (- 35m 22s) (35000 46%) 1.6478\n",
            "31m 2s (- 35m 16s) (35100 46%) 1.4701\n",
            "31m 7s (- 35m 11s) (35200 46%) 1.5801\n",
            "31m 12s (- 35m 6s) (35300 47%) 1.5187\n",
            "31m 18s (- 35m 0s) (35400 47%) 1.6420\n",
            "31m 23s (- 34m 56s) (35500 47%) 1.5985\n",
            "31m 29s (- 34m 50s) (35600 47%) 1.6392\n",
            "31m 34s (- 34m 45s) (35700 47%) 1.4293\n",
            "31m 39s (- 34m 40s) (35800 47%) 1.5829\n",
            "31m 45s (- 34m 35s) (35900 47%) 1.5493\n",
            "31m 50s (- 34m 29s) (36000 48%) 1.5765\n",
            "31m 56s (- 34m 24s) (36100 48%) 1.4437\n",
            "32m 1s (- 34m 19s) (36200 48%) 1.5854\n",
            "32m 6s (- 34m 14s) (36300 48%) 1.5605\n",
            "32m 12s (- 34m 8s) (36400 48%) 1.5757\n",
            "32m 17s (- 34m 3s) (36500 48%) 1.5583\n",
            "32m 23s (- 33m 58s) (36600 48%) 1.6206\n",
            "32m 28s (- 33m 53s) (36700 48%) 1.7115\n",
            "32m 33s (- 33m 48s) (36800 49%) 1.5180\n",
            "32m 39s (- 33m 43s) (36900 49%) 1.4765\n",
            "32m 44s (- 33m 37s) (37000 49%) 1.5080\n",
            "32m 49s (- 33m 32s) (37100 49%) 1.3957\n",
            "32m 55s (- 33m 27s) (37200 49%) 1.6923\n",
            "33m 0s (- 33m 21s) (37300 49%) 1.4967\n",
            "33m 6s (- 33m 16s) (37400 49%) 1.5625\n",
            "33m 11s (- 33m 11s) (37500 50%) 1.4858\n",
            "33m 16s (- 33m 5s) (37600 50%) 1.6109\n",
            "33m 22s (- 33m 0s) (37700 50%) 1.3827\n",
            "33m 27s (- 32m 55s) (37800 50%) 1.7410\n",
            "33m 32s (- 32m 50s) (37900 50%) 1.5097\n",
            "33m 38s (- 32m 45s) (38000 50%) 1.6267\n",
            "33m 43s (- 32m 39s) (38100 50%) 1.4940\n",
            "33m 48s (- 32m 34s) (38200 50%) 1.6273\n",
            "33m 54s (- 32m 29s) (38300 51%) 1.5519\n",
            "33m 59s (- 32m 24s) (38400 51%) 1.6066\n",
            "34m 5s (- 32m 18s) (38500 51%) 1.7721\n",
            "34m 10s (- 32m 13s) (38600 51%) 1.4221\n",
            "34m 15s (- 32m 8s) (38700 51%) 1.4234\n",
            "34m 20s (- 32m 2s) (38800 51%) 1.5463\n",
            "34m 26s (- 31m 57s) (38900 51%) 1.5717\n",
            "34m 31s (- 31m 52s) (39000 52%) 1.3802\n",
            "34m 37s (- 31m 47s) (39100 52%) 1.4230\n",
            "34m 43s (- 31m 42s) (39200 52%) 1.6292\n",
            "34m 48s (- 31m 37s) (39300 52%) 1.5910\n",
            "34m 54s (- 31m 32s) (39400 52%) 1.4259\n",
            "34m 59s (- 31m 26s) (39500 52%) 1.5564\n",
            "35m 4s (- 31m 21s) (39600 52%) 1.7366\n",
            "35m 10s (- 31m 16s) (39700 52%) 1.6090\n",
            "35m 15s (- 31m 10s) (39800 53%) 1.4441\n",
            "35m 20s (- 31m 5s) (39900 53%) 1.5249\n",
            "35m 26s (- 31m 0s) (40000 53%) 1.5713\n",
            "35m 31s (- 30m 55s) (40100 53%) 1.4935\n",
            "35m 37s (- 30m 49s) (40200 53%) 1.4874\n",
            "35m 42s (- 30m 44s) (40300 53%) 1.7128\n",
            "35m 48s (- 30m 39s) (40400 53%) 1.3955\n",
            "35m 53s (- 30m 34s) (40500 54%) 1.4414\n",
            "35m 58s (- 30m 29s) (40600 54%) 1.3927\n",
            "36m 4s (- 30m 23s) (40700 54%) 1.4883\n",
            "36m 9s (- 30m 18s) (40800 54%) 1.5619\n",
            "36m 15s (- 30m 13s) (40900 54%) 1.4419\n",
            "36m 20s (- 30m 8s) (41000 54%) 1.6548\n",
            "36m 26s (- 30m 3s) (41100 54%) 1.4518\n",
            "36m 31s (- 29m 57s) (41200 54%) 1.5813\n",
            "36m 36s (- 29m 52s) (41300 55%) 1.5846\n",
            "36m 41s (- 29m 47s) (41400 55%) 1.5486\n",
            "36m 47s (- 29m 41s) (41500 55%) 1.6959\n",
            "36m 52s (- 29m 36s) (41600 55%) 1.5887\n",
            "36m 57s (- 29m 30s) (41700 55%) 1.5252\n",
            "37m 2s (- 29m 25s) (41800 55%) 1.5073\n",
            "37m 8s (- 29m 20s) (41900 55%) 1.5143\n",
            "37m 13s (- 29m 14s) (42000 56%) 1.6529\n",
            "37m 18s (- 29m 8s) (42100 56%) 1.6070\n",
            "37m 23s (- 29m 3s) (42200 56%) 1.6996\n",
            "37m 29s (- 28m 58s) (42300 56%) 1.5923\n",
            "37m 34s (- 28m 53s) (42400 56%) 1.3800\n",
            "37m 39s (- 28m 48s) (42500 56%) 1.5848\n",
            "37m 45s (- 28m 43s) (42600 56%) 1.7386\n",
            "37m 50s (- 28m 37s) (42700 56%) 1.5853\n",
            "37m 55s (- 28m 32s) (42800 57%) 1.4729\n",
            "38m 1s (- 28m 27s) (42900 57%) 1.5431\n",
            "38m 6s (- 28m 21s) (43000 57%) 1.5490\n",
            "38m 12s (- 28m 16s) (43100 57%) 1.4502\n",
            "38m 17s (- 28m 11s) (43200 57%) 1.5219\n",
            "38m 23s (- 28m 6s) (43300 57%) 1.5692\n",
            "38m 28s (- 28m 1s) (43400 57%) 1.7896\n",
            "38m 34s (- 27m 56s) (43500 57%) 1.5848\n",
            "38m 40s (- 27m 50s) (43600 58%) 1.6512\n",
            "38m 45s (- 27m 45s) (43700 58%) 1.5773\n",
            "38m 51s (- 27m 40s) (43800 58%) 1.4355\n",
            "38m 56s (- 27m 35s) (43900 58%) 1.5273\n",
            "39m 1s (- 27m 29s) (44000 58%) 1.6302\n",
            "39m 7s (- 27m 24s) (44100 58%) 1.4554\n",
            "39m 12s (- 27m 19s) (44200 58%) 1.5971\n",
            "39m 17s (- 27m 13s) (44300 59%) 1.3471\n",
            "39m 22s (- 27m 8s) (44400 59%) 1.3381\n",
            "39m 28s (- 27m 3s) (44500 59%) 1.6497\n",
            "39m 33s (- 26m 57s) (44600 59%) 1.4040\n",
            "39m 39s (- 26m 52s) (44700 59%) 1.5624\n",
            "39m 44s (- 26m 47s) (44800 59%) 1.4165\n",
            "39m 50s (- 26m 42s) (44900 59%) 1.6161\n",
            "39m 55s (- 26m 36s) (45000 60%) 1.5892\n",
            "40m 0s (- 26m 31s) (45100 60%) 1.4488\n",
            "40m 5s (- 26m 26s) (45200 60%) 1.5356\n",
            "40m 11s (- 26m 21s) (45300 60%) 1.6651\n",
            "40m 16s (- 26m 15s) (45400 60%) 1.5638\n",
            "40m 22s (- 26m 10s) (45500 60%) 1.5173\n",
            "40m 27s (- 26m 5s) (45600 60%) 1.5865\n",
            "40m 33s (- 26m 0s) (45700 60%) 1.5080\n",
            "40m 38s (- 25m 54s) (45800 61%) 1.3093\n",
            "40m 44s (- 25m 49s) (45900 61%) 1.7447\n",
            "40m 49s (- 25m 44s) (46000 61%) 1.3530\n",
            "40m 54s (- 25m 38s) (46100 61%) 1.5705\n",
            "41m 0s (- 25m 33s) (46200 61%) 1.4955\n",
            "41m 5s (- 25m 28s) (46300 61%) 1.6577\n",
            "41m 11s (- 25m 23s) (46400 61%) 1.6468\n",
            "41m 16s (- 25m 18s) (46500 62%) 1.6595\n",
            "41m 22s (- 25m 12s) (46600 62%) 1.5649\n",
            "41m 27s (- 25m 7s) (46700 62%) 1.6160\n",
            "41m 33s (- 25m 2s) (46800 62%) 1.5499\n",
            "41m 38s (- 24m 57s) (46900 62%) 1.4428\n",
            "41m 43s (- 24m 51s) (47000 62%) 1.4963\n",
            "41m 49s (- 24m 46s) (47100 62%) 1.5559\n",
            "41m 54s (- 24m 41s) (47200 62%) 1.4018\n",
            "42m 0s (- 24m 35s) (47300 63%) 1.6317\n",
            "42m 5s (- 24m 30s) (47400 63%) 1.5038\n",
            "42m 10s (- 24m 25s) (47500 63%) 1.6814\n",
            "42m 16s (- 24m 19s) (47600 63%) 1.4246\n",
            "42m 21s (- 24m 14s) (47700 63%) 1.4874\n",
            "42m 26s (- 24m 9s) (47800 63%) 1.6706\n",
            "42m 31s (- 24m 3s) (47900 63%) 1.5253\n",
            "42m 37s (- 23m 58s) (48000 64%) 1.5937\n",
            "42m 43s (- 23m 53s) (48100 64%) 1.5194\n",
            "42m 48s (- 23m 48s) (48200 64%) 1.6167\n",
            "42m 54s (- 23m 43s) (48300 64%) 1.3734\n",
            "42m 59s (- 23m 37s) (48400 64%) 1.5964\n",
            "43m 5s (- 23m 32s) (48500 64%) 1.5838\n",
            "43m 10s (- 23m 27s) (48600 64%) 1.4910\n",
            "43m 15s (- 23m 21s) (48700 64%) 1.4285\n",
            "43m 21s (- 23m 16s) (48800 65%) 1.4502\n",
            "43m 26s (- 23m 11s) (48900 65%) 1.6118\n",
            "43m 32s (- 23m 6s) (49000 65%) 1.5969\n",
            "43m 37s (- 23m 0s) (49100 65%) 1.5057\n",
            "43m 43s (- 22m 55s) (49200 65%) 1.5957\n",
            "43m 49s (- 22m 50s) (49300 65%) 1.5320\n",
            "43m 54s (- 22m 45s) (49400 65%) 1.4393\n",
            "44m 0s (- 22m 40s) (49500 66%) 1.5504\n",
            "44m 5s (- 22m 34s) (49600 66%) 1.4361\n",
            "44m 11s (- 22m 29s) (49700 66%) 1.6260\n",
            "44m 16s (- 22m 24s) (49800 66%) 1.6019\n",
            "44m 22s (- 22m 19s) (49900 66%) 1.4617\n",
            "44m 27s (- 22m 13s) (50000 66%) 1.5379\n",
            "44m 33s (- 22m 8s) (50100 66%) 1.6138\n",
            "44m 39s (- 22m 3s) (50200 66%) 1.3945\n",
            "44m 44s (- 21m 58s) (50300 67%) 1.4488\n",
            "44m 50s (- 21m 53s) (50400 67%) 1.5309\n",
            "44m 55s (- 21m 47s) (50500 67%) 1.6860\n",
            "45m 1s (- 21m 42s) (50600 67%) 1.5721\n",
            "45m 7s (- 21m 37s) (50700 67%) 1.6068\n",
            "45m 12s (- 21m 32s) (50800 67%) 1.5997\n",
            "45m 18s (- 21m 26s) (50900 67%) 1.5988\n",
            "45m 23s (- 21m 21s) (51000 68%) 1.5767\n",
            "45m 28s (- 21m 16s) (51100 68%) 1.7002\n",
            "45m 34s (- 21m 10s) (51200 68%) 1.5268\n",
            "45m 39s (- 21m 5s) (51300 68%) 1.3819\n",
            "45m 44s (- 21m 0s) (51400 68%) 1.6088\n",
            "45m 50s (- 20m 55s) (51500 68%) 1.5382\n",
            "45m 55s (- 20m 49s) (51600 68%) 1.5889\n",
            "46m 0s (- 20m 44s) (51700 68%) 1.3657\n",
            "46m 6s (- 20m 38s) (51800 69%) 1.5582\n",
            "46m 11s (- 20m 33s) (51900 69%) 1.6822\n",
            "46m 16s (- 20m 28s) (52000 69%) 1.5386\n",
            "46m 22s (- 20m 23s) (52100 69%) 1.6921\n",
            "46m 28s (- 20m 17s) (52200 69%) 1.3949\n",
            "46m 33s (- 20m 12s) (52300 69%) 1.4677\n",
            "46m 38s (- 20m 7s) (52400 69%) 1.5689\n",
            "46m 43s (- 20m 1s) (52500 70%) 1.6005\n",
            "46m 49s (- 19m 56s) (52600 70%) 1.8113\n",
            "46m 54s (- 19m 50s) (52700 70%) 1.4762\n",
            "47m 0s (- 19m 45s) (52800 70%) 1.5078\n",
            "47m 5s (- 19m 40s) (52900 70%) 1.6550\n",
            "47m 10s (- 19m 35s) (53000 70%) 1.4463\n",
            "47m 16s (- 19m 29s) (53100 70%) 1.7602\n",
            "47m 22s (- 19m 24s) (53200 70%) 1.5946\n",
            "47m 27s (- 19m 19s) (53300 71%) 1.4440\n",
            "47m 33s (- 19m 14s) (53400 71%) 1.4466\n",
            "47m 39s (- 19m 8s) (53500 71%) 1.7051\n",
            "47m 44s (- 19m 3s) (53600 71%) 1.7761\n",
            "47m 50s (- 18m 58s) (53700 71%) 1.5308\n",
            "47m 55s (- 18m 53s) (53800 71%) 1.7495\n",
            "48m 1s (- 18m 47s) (53900 71%) 1.6238\n",
            "48m 6s (- 18m 42s) (54000 72%) 1.7181\n",
            "48m 12s (- 18m 37s) (54100 72%) 1.5674\n",
            "48m 17s (- 18m 32s) (54200 72%) 1.6526\n",
            "48m 23s (- 18m 26s) (54300 72%) 1.5143\n",
            "48m 28s (- 18m 21s) (54400 72%) 1.5715\n",
            "48m 33s (- 18m 16s) (54500 72%) 1.5491\n",
            "48m 39s (- 18m 10s) (54600 72%) 1.6777\n",
            "48m 44s (- 18m 5s) (54700 72%) 1.5863\n",
            "48m 50s (- 18m 0s) (54800 73%) 1.6879\n",
            "48m 55s (- 17m 54s) (54900 73%) 1.3992\n",
            "49m 1s (- 17m 49s) (55000 73%) 1.5915\n",
            "49m 6s (- 17m 44s) (55100 73%) 1.6102\n",
            "49m 12s (- 17m 38s) (55200 73%) 1.2959\n",
            "49m 18s (- 17m 33s) (55300 73%) 1.7019\n",
            "49m 23s (- 17m 28s) (55400 73%) 1.4720\n",
            "49m 28s (- 17m 23s) (55500 74%) 1.5374\n",
            "49m 34s (- 17m 17s) (55600 74%) 1.5654\n",
            "49m 39s (- 17m 12s) (55700 74%) 1.5948\n",
            "49m 45s (- 17m 7s) (55800 74%) 1.5234\n",
            "49m 50s (- 17m 1s) (55900 74%) 1.5517\n",
            "49m 56s (- 16m 56s) (56000 74%) 1.6448\n",
            "50m 1s (- 16m 51s) (56100 74%) 1.4822\n",
            "50m 6s (- 16m 45s) (56200 74%) 1.5411\n",
            "50m 11s (- 16m 40s) (56300 75%) 1.6116\n",
            "50m 17s (- 16m 35s) (56400 75%) 1.6908\n",
            "50m 21s (- 16m 29s) (56500 75%) 1.4567\n",
            "50m 27s (- 16m 24s) (56600 75%) 1.5645\n",
            "50m 32s (- 16m 18s) (56700 75%) 1.4871\n",
            "50m 38s (- 16m 13s) (56800 75%) 1.5281\n",
            "50m 43s (- 16m 8s) (56900 75%) 1.5344\n",
            "50m 48s (- 16m 2s) (57000 76%) 1.6063\n",
            "50m 54s (- 15m 57s) (57100 76%) 1.4503\n",
            "50m 59s (- 15m 52s) (57200 76%) 1.5565\n",
            "51m 4s (- 15m 46s) (57300 76%) 1.6604\n",
            "51m 10s (- 15m 41s) (57400 76%) 1.3348\n",
            "51m 15s (- 15m 36s) (57500 76%) 1.5246\n",
            "51m 20s (- 15m 30s) (57600 76%) 1.3961\n",
            "51m 26s (- 15m 25s) (57700 76%) 1.5766\n",
            "51m 32s (- 15m 20s) (57800 77%) 1.6667\n",
            "51m 37s (- 15m 14s) (57900 77%) 1.3998\n",
            "51m 42s (- 15m 9s) (58000 77%) 1.5845\n",
            "51m 48s (- 15m 4s) (58100 77%) 1.3346\n",
            "51m 53s (- 14m 58s) (58200 77%) 1.5987\n",
            "51m 59s (- 14m 53s) (58300 77%) 1.5966\n",
            "52m 4s (- 14m 48s) (58400 77%) 1.4984\n",
            "52m 10s (- 14m 42s) (58500 78%) 1.6485\n",
            "52m 15s (- 14m 37s) (58600 78%) 1.6992\n",
            "52m 21s (- 14m 32s) (58700 78%) 1.6330\n",
            "52m 26s (- 14m 26s) (58800 78%) 1.2446\n",
            "52m 32s (- 14m 21s) (58900 78%) 1.5122\n",
            "52m 37s (- 14m 16s) (59000 78%) 1.5109\n",
            "52m 43s (- 14m 10s) (59100 78%) 1.3837\n",
            "52m 48s (- 14m 5s) (59200 78%) 1.6589\n",
            "52m 54s (- 14m 0s) (59300 79%) 1.4989\n",
            "52m 59s (- 13m 55s) (59400 79%) 1.5006\n",
            "53m 5s (- 13m 49s) (59500 79%) 1.7267\n",
            "53m 10s (- 13m 44s) (59600 79%) 1.5392\n",
            "53m 16s (- 13m 39s) (59700 79%) 1.5058\n",
            "53m 21s (- 13m 33s) (59800 79%) 1.3584\n",
            "53m 27s (- 13m 28s) (59900 79%) 1.5552\n",
            "53m 33s (- 13m 23s) (60000 80%) 1.7404\n",
            "53m 38s (- 13m 18s) (60100 80%) 1.5518\n",
            "53m 44s (- 13m 12s) (60200 80%) 1.5841\n",
            "53m 49s (- 13m 7s) (60300 80%) 1.4592\n",
            "53m 54s (- 13m 1s) (60400 80%) 1.5447\n",
            "54m 0s (- 12m 56s) (60500 80%) 1.6710\n",
            "54m 5s (- 12m 51s) (60600 80%) 1.3773\n",
            "54m 11s (- 12m 46s) (60700 80%) 1.5526\n",
            "54m 17s (- 12m 40s) (60800 81%) 1.6292\n",
            "54m 23s (- 12m 35s) (60900 81%) 1.5608\n",
            "54m 29s (- 12m 30s) (61000 81%) 1.6022\n",
            "54m 34s (- 12m 24s) (61100 81%) 1.5483\n",
            "54m 40s (- 12m 19s) (61200 81%) 1.3644\n",
            "54m 45s (- 12m 14s) (61300 81%) 1.4411\n",
            "54m 51s (- 12m 8s) (61400 81%) 1.5028\n",
            "54m 56s (- 12m 3s) (61500 82%) 1.5729\n",
            "55m 2s (- 11m 58s) (61600 82%) 1.5345\n",
            "55m 7s (- 11m 53s) (61700 82%) 1.6312\n",
            "55m 13s (- 11m 47s) (61800 82%) 1.5217\n",
            "55m 19s (- 11m 42s) (61900 82%) 1.5036\n",
            "55m 24s (- 11m 37s) (62000 82%) 1.5123\n",
            "55m 30s (- 11m 31s) (62100 82%) 1.5393\n",
            "55m 35s (- 11m 26s) (62200 82%) 1.5490\n",
            "55m 40s (- 11m 20s) (62300 83%) 1.4329\n",
            "55m 45s (- 11m 15s) (62400 83%) 1.5463\n",
            "55m 50s (- 11m 10s) (62500 83%) 1.3058\n",
            "55m 55s (- 11m 4s) (62600 83%) 1.5451\n",
            "56m 1s (- 10m 59s) (62700 83%) 1.4478\n",
            "56m 6s (- 10m 53s) (62800 83%) 1.3994\n",
            "56m 11s (- 10m 48s) (62900 83%) 1.5989\n",
            "56m 16s (- 10m 43s) (63000 84%) 1.5590\n",
            "56m 21s (- 10m 37s) (63100 84%) 1.2870\n",
            "56m 27s (- 10m 32s) (63200 84%) 1.5180\n",
            "56m 32s (- 10m 27s) (63300 84%) 1.4847\n",
            "56m 38s (- 10m 21s) (63400 84%) 1.7450\n",
            "56m 43s (- 10m 16s) (63500 84%) 1.4896\n",
            "56m 49s (- 10m 11s) (63600 84%) 1.2794\n",
            "56m 54s (- 10m 5s) (63700 84%) 1.4530\n",
            "57m 0s (- 10m 0s) (63800 85%) 1.4618\n",
            "57m 5s (- 9m 55s) (63900 85%) 1.5631\n",
            "57m 10s (- 9m 49s) (64000 85%) 1.5541\n",
            "57m 15s (- 9m 44s) (64100 85%) 1.4046\n",
            "57m 21s (- 9m 38s) (64200 85%) 1.3791\n",
            "57m 26s (- 9m 33s) (64300 85%) 1.5426\n",
            "57m 31s (- 9m 28s) (64400 85%) 1.5310\n",
            "57m 36s (- 9m 22s) (64500 86%) 1.4410\n",
            "57m 42s (- 9m 17s) (64600 86%) 1.6123\n",
            "57m 47s (- 9m 12s) (64700 86%) 1.4409\n",
            "57m 53s (- 9m 6s) (64800 86%) 1.6775\n",
            "57m 58s (- 9m 1s) (64900 86%) 1.3276\n",
            "58m 3s (- 8m 55s) (65000 86%) 1.6344\n",
            "58m 9s (- 8m 50s) (65100 86%) 1.5787\n",
            "58m 14s (- 8m 45s) (65200 86%) 1.5316\n",
            "58m 19s (- 8m 39s) (65300 87%) 1.7497\n",
            "58m 24s (- 8m 34s) (65400 87%) 1.5842\n",
            "58m 30s (- 8m 29s) (65500 87%) 1.3915\n",
            "58m 35s (- 8m 23s) (65600 87%) 1.7348\n",
            "58m 40s (- 8m 18s) (65700 87%) 1.4717\n",
            "58m 45s (- 8m 12s) (65800 87%) 1.4920\n",
            "58m 51s (- 8m 7s) (65900 87%) 1.6539\n",
            "58m 56s (- 8m 2s) (66000 88%) 1.5620\n",
            "59m 2s (- 7m 56s) (66100 88%) 1.4812\n",
            "59m 7s (- 7m 51s) (66200 88%) 1.4167\n",
            "59m 12s (- 7m 46s) (66300 88%) 1.4509\n",
            "59m 17s (- 7m 40s) (66400 88%) 1.5496\n",
            "59m 23s (- 7m 35s) (66500 88%) 1.7341\n",
            "59m 28s (- 7m 30s) (66600 88%) 1.3922\n",
            "59m 33s (- 7m 24s) (66700 88%) 1.4944\n",
            "59m 38s (- 7m 19s) (66800 89%) 1.4932\n",
            "59m 43s (- 7m 13s) (66900 89%) 1.5186\n",
            "59m 49s (- 7m 8s) (67000 89%) 1.6169\n",
            "59m 54s (- 7m 3s) (67100 89%) 1.4313\n",
            "60m 0s (- 6m 57s) (67200 89%) 1.7185\n",
            "60m 5s (- 6m 52s) (67300 89%) 1.5914\n",
            "60m 10s (- 6m 47s) (67400 89%) 1.5439\n",
            "60m 16s (- 6m 41s) (67500 90%) 1.5878\n",
            "60m 21s (- 6m 36s) (67600 90%) 1.5730\n",
            "60m 27s (- 6m 31s) (67700 90%) 1.4967\n",
            "60m 32s (- 6m 25s) (67800 90%) 1.7032\n",
            "60m 38s (- 6m 20s) (67900 90%) 1.6089\n",
            "60m 43s (- 6m 15s) (68000 90%) 1.5620\n",
            "60m 48s (- 6m 9s) (68100 90%) 1.3918\n",
            "60m 54s (- 6m 4s) (68200 90%) 1.7689\n",
            "60m 59s (- 5m 58s) (68300 91%) 1.4312\n",
            "61m 4s (- 5m 53s) (68400 91%) 1.6531\n",
            "61m 10s (- 5m 48s) (68500 91%) 1.5305\n",
            "61m 15s (- 5m 42s) (68600 91%) 1.5934\n",
            "61m 21s (- 5m 37s) (68700 91%) 1.5329\n",
            "61m 26s (- 5m 32s) (68800 91%) 1.6992\n",
            "61m 31s (- 5m 26s) (68900 91%) 1.5715\n",
            "61m 37s (- 5m 21s) (69000 92%) 1.5456\n",
            "61m 42s (- 5m 16s) (69100 92%) 1.8872\n",
            "61m 48s (- 5m 10s) (69200 92%) 1.7140\n",
            "61m 53s (- 5m 5s) (69300 92%) 1.7321\n",
            "61m 59s (- 5m 0s) (69400 92%) 1.7079\n",
            "62m 4s (- 4m 54s) (69500 92%) 1.6803\n",
            "62m 9s (- 4m 49s) (69600 92%) 1.4511\n",
            "62m 14s (- 4m 43s) (69700 92%) 1.4336\n",
            "62m 20s (- 4m 38s) (69800 93%) 1.7092\n",
            "62m 25s (- 4m 33s) (69900 93%) 1.6032\n",
            "62m 30s (- 4m 27s) (70000 93%) 1.5004\n",
            "62m 36s (- 4m 22s) (70100 93%) 1.5856\n",
            "62m 42s (- 4m 17s) (70200 93%) 1.6492\n",
            "62m 47s (- 4m 11s) (70300 93%) 1.6316\n",
            "62m 53s (- 4m 6s) (70400 93%) 1.6238\n",
            "62m 58s (- 4m 1s) (70500 94%) 1.9383\n",
            "63m 3s (- 3m 55s) (70600 94%) 1.6110\n",
            "63m 9s (- 3m 50s) (70700 94%) 1.6615\n",
            "63m 14s (- 3m 45s) (70800 94%) 1.6805\n",
            "63m 20s (- 3m 39s) (70900 94%) 1.6700\n",
            "63m 25s (- 3m 34s) (71000 94%) 1.7383\n",
            "63m 31s (- 3m 29s) (71100 94%) 1.7019\n",
            "63m 36s (- 3m 23s) (71200 94%) 1.5381\n",
            "63m 41s (- 3m 18s) (71300 95%) 1.6290\n",
            "63m 47s (- 3m 12s) (71400 95%) 1.6543\n",
            "63m 53s (- 3m 7s) (71500 95%) 1.9230\n",
            "63m 58s (- 3m 2s) (71600 95%) 1.7953\n",
            "64m 3s (- 2m 56s) (71700 95%) 1.5731\n",
            "64m 8s (- 2m 51s) (71800 95%) 1.7869\n",
            "64m 13s (- 2m 46s) (71900 95%) 1.6975\n",
            "64m 19s (- 2m 40s) (72000 96%) 1.8060\n",
            "64m 25s (- 2m 35s) (72100 96%) 1.8031\n",
            "64m 30s (- 2m 30s) (72200 96%) 1.7502\n",
            "64m 36s (- 2m 24s) (72300 96%) 1.7826\n",
            "64m 41s (- 2m 19s) (72400 96%) 1.5181\n",
            "64m 46s (- 2m 14s) (72500 96%) 1.6706\n",
            "64m 51s (- 2m 8s) (72600 96%) 1.5475\n",
            "64m 57s (- 2m 3s) (72700 96%) 1.7341\n",
            "65m 2s (- 1m 57s) (72800 97%) 1.5990\n",
            "65m 7s (- 1m 52s) (72900 97%) 1.7854\n",
            "65m 12s (- 1m 47s) (73000 97%) 1.7212\n",
            "65m 18s (- 1m 41s) (73100 97%) 1.7302\n",
            "65m 23s (- 1m 36s) (73200 97%) 1.6412\n",
            "65m 28s (- 1m 31s) (73300 97%) 1.6871\n",
            "65m 34s (- 1m 25s) (73400 97%) 1.6982\n",
            "65m 40s (- 1m 20s) (73500 98%) 1.8546\n",
            "65m 45s (- 1m 15s) (73600 98%) 1.8385\n",
            "65m 50s (- 1m 9s) (73700 98%) 1.5221\n",
            "65m 55s (- 1m 4s) (73800 98%) 1.4386\n",
            "66m 1s (- 0m 58s) (73900 98%) 1.8410\n",
            "66m 6s (- 0m 53s) (74000 98%) 1.9519\n",
            "66m 12s (- 0m 48s) (74100 98%) 1.5992\n",
            "66m 17s (- 0m 42s) (74200 98%) 1.7399\n",
            "66m 22s (- 0m 37s) (74300 99%) 1.7601\n",
            "66m 27s (- 0m 32s) (74400 99%) 1.7436\n",
            "66m 33s (- 0m 26s) (74500 99%) 1.8899\n",
            "66m 38s (- 0m 21s) (74600 99%) 1.5551\n",
            "66m 44s (- 0m 16s) (74700 99%) 1.7578\n",
            "66m 49s (- 0m 10s) (74800 99%) 1.7447\n",
            "66m 54s (- 0m 5s) (74900 99%) 1.8193\n",
            "66m 59s (- 0m 0s) (75000 100%) 1.3986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfCQauf3OeaC",
        "colab_type": "code",
        "outputId": "2e994b9a-4aed-485e-c282-4e1f736ce190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "showPlot(train_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9u3eQslGxTW",
        "colab_type": "code",
        "outputId": "57abebef-70f6-40eb-8754-8bb7d586672c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1, pairs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> print two numbers `10` and `20` using string formatting\n",
            "= \"\"\"{0} {1}\"\"\".format(10, 20)\n",
            "< \"\"\"{0{}\"\".join(())\n",
            "\n",
            "> output first 100 characters in a string `my_string`\n",
            "= print(my_string[0:100])\n",
            "< print(my_string.string()string)string:100[:100])\n",
            "\n",
            "> concatenate dataframe `df1` with `df2` whilst removing duplicates\n",
            "= pandas.concat([df1, df2]).drop_duplicates().reset_index(drop=True)\n",
            "< pandas.concat([df1[reset')]drop)\n",
            "\n",
            "> Sort dictionary `dict1` by value in ascending order\n",
            "= sorted(dict1, key=dict1.get)\n",
            "< sorted(dict1( key=dict1(d.get))\n",
            "\n",
            "> printing numbers rounding up to third decimal place\n",
            "= print('%.3f' % 3.1415)\n",
            "< print('%join'.joinjoinjoinjoinjoinjoinjoinjoinjoinjoin('3f'\n",
            "\n",
            "> Get the zip output as list from the lists `[1, 2, 3]`, `[4, 5, 6]`, `[7, 8, 9]`\n",
            "= [list(a) for a in zip([1, 2, 3], [4, 5, 6], [7, 8, 9])]\n",
            "< [(a[0]((((((])(,,,,])(,,\n",
            "\n",
            "> get the ASCII value of a character as an int\n",
            "= ord()\n",
            "< ord()(u3042(u3042()\n",
            "\n",
            "> sort list `['14:10:01', '03:12:08']`\n",
            "= sorted(['14:10:01', '03:12:08'])\n",
            "< sorted(['14'14:'01':1010:10', ':10:')\n",
            "\n",
            "> find the current directory\n",
            "= os.path.realpath(__file__)\n",
            "< os.path.realpath(____=\n",
            "\n",
            "> match a sharp, followed by letters (including accent characters) in string `str1` using a regex\n",
            "= hashtags = re.findall('#(\\\\w+)', str1, re.UNICODE)\n",
            "< hashtags = re.findall('\\\\\\+\\\\, '\\\\ ')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VheXuNeG4E_k",
        "colab_type": "code",
        "outputId": "8e18df70-5aea-4306-86bb-c28c1a71255c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_answers=evaluate_data_set(encoder1, attn_decoder1, pairs)\n",
        "acc=0\n",
        "for i in range(len(train_answers)):\n",
        "  if pairs[i][1]==train_answers[i]:\n",
        "    acc+=1\n",
        "print(\"Correct on {} snippets out of {} on training set\".format(acc, len(pairs)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> Concatenate elements of a list 'x' of multiple integers to a single integer\n",
            "= sum(d * 10 ** i for i, d in enumerate(x[::-1]))\n",
            "< sum(d , 10 ) 10 * 10 * 10 * 10 ,(x[1]),),))<EOS>\n",
            "\n",
            "> convert list `l` to dictionary having each two adjacent elements as key/value pair\n",
            "= dict(zip(l[::2], l[1::2]))\n",
            "< dict(zip(l:::::::::::::::<EOS>\n",
            "\n",
            "> replace white spaces in dataframe `df` with '_'\n",
            "= df.replace(' ', '_', regex=True)\n",
            "< df.replace(', ', ')=', ')<EOS>\n",
            "\n",
            "> Return a subplot axes positioned by the grid definition `1,1,1` using matpotlib\n",
            "= fig.add_subplot(1, 1, 1)\n",
            "< datetime.add_subplot(', 1/ 1.1,', ')<EOS>\n",
            "\n",
            "> match the pattern '[:;][)(](?![)(])' to the string `str`\n",
            "= re.match('[:;][)(](?![)(])', str)\n",
            "< re.match('[(]((]((( str(,, str(,, s)<EOS>\n",
            "\n",
            "> trim whitespace in string `s`\n",
            "= s.strip()\n",
            "< s[strip()<EOS>\n",
            "\n",
            "> Get the first and last 3 elements of list `l`\n",
            "= l[:3] + l[-3:]\n",
            "< l[: + l[+]<EOS>\n",
            "\n",
            "> format a string `num` using string formatting\n",
            "= \"\"\"{0:.3g}\"\"\".format(num)\n",
            "< \"\"\"{0:3g<.}\"(\".()<EOS>\n",
            "\n",
            "> regular expression match nothing\n",
            "= re.compile('$^')\n",
            "< re.compile('^<EOS>\n",
            "\n",
            "> reverse sort items in default dictionary `cityPopulation` by the third item in each key's list of values\n",
            "= sorted(iter(cityPopulation.items()), key=lambda k_v: k_v[1][2], reverse=True)\n",
            "< sorted(iter(cityPopulation.items()), key=lambda k_v[1, 2)]<EOS>\n",
            "\n",
            "> sort list `lst` in descending order based on the second item of each tuple in it\n",
            "= lst.sort(key=lambda x: x[2], reverse=True)\n",
            "< sorted.sort(key=lambda x: x[0].values( reverse=True))<EOS>\n",
            "\n",
            "> print string \"ABC\" as hex literal\n",
            "= \"\"\"\u0001ABC\"\"\".encode('hex')\n",
            "< \"\"\"\"\"\"\".join(''.()'-<EOS>\n",
            "\n",
            "> Create 2D numpy array from the data provided in 'somefile.csv' with each row in the file having same number of values\n",
            "= X = numpy.loadtxt('somefile.csv', delimiter=',')\n",
            "< X = numpy=loadtxt(loadtxt(loadtxt(loadtxt.csv('=(_==)<EOS>\n",
            "\n",
            "> convert hex string 'deadbeef' to decimal\n",
            "= int('deadbeef', 16)\n",
            "< int('deadbeef', 16)<EOS>\n",
            "\n",
            "> remove the punctuation '!', '.', ':' from a string `asking`\n",
            "= out = ''.join(c for c in asking if c not in ('!', '.', ':'))\n",
            "< out = ''c for c in asking if c not in ', '', ''')( '')<EOS>\n",
            "\n",
            "> convert string `x'  to dictionary splitted by `=` using list comprehension\n",
            "= dict([x.split('=') for x in s.split()])\n",
            "< dict(x['id'] for x in s] for x in s'], x)<EOS>\n",
            "\n",
            "> Check if the value of the key \"name\" is \"Test\" in a list of dictionaries `label`\n",
            "= any(d['name'] == 'Test' for d in label)\n",
            "< any(d[ 'name'] == 1)<EOS>\n",
            "\n",
            "> create list `levels` containing 3 empty dictionaries\n",
            "= levels = [{}, {}, {}]\n",
            "< levels = [{,}, {'}, {'}', 3)<EOS>\n",
            "\n",
            "> reverse a list `L`\n",
            "= L[::(-1)]\n",
            "< L.reverse('(<EOS>\n",
            "\n",
            "> get the middle two characters of a string 'state' in a pandas dataframe `df`\n",
            "= df['state'].apply(lambda x: x[len(x) / 2 - 1:len(x) / 2 + 1])\n",
            "< df['state'[:: (len(len(len(len(x) / 2 -:]) // 2 /len())<EOS>\n",
            "\n",
            "> sort a pandas data frame by column `Peak` in ascending and `Weeks` in descending order\n",
            "= df.sort(['Peak', 'Weeks'], ascending=[True, False], inplace=True)\n",
            "< df.sort(['Peak', 'Weeks', ascending=','], inplace=True, False)<EOS>\n",
            "\n",
            "> convert datetime object to date object in python\n",
            "= datetime.datetime.now().date()\n",
            "< datetime.datetime.now('-.',:))<EOS>\n",
            "\n",
            "> Get all texts and tags from a tag `strong` from etree tag `some_tag` using lxml\n",
            "= print(etree.tostring(some_tag.find('strong')))\n",
            "< print('etree'.find('''strong','')))<EOS>\n",
            "\n",
            "> remove trailing newline in string \"test string\\n\"\n",
            "= 'test string\\n'.rstrip()\n",
            "< 'test string')<EOS>\n",
            "\n",
            "> Print variable `count` and variable `conv` with space string '    ' in between\n",
            "= print(str(count) + '    ' + str(conv))\n",
            "< print(str.count(str( ')<EOS>\n",
            "\n",
            "> parse a YAML file \"example.yaml\"\n",
            "= with open('example.yaml') as stream:\n",
            "    try:\n",
            "        print((yaml.load(stream)))\n",
            "    except yaml.YAMLError as exc:\n",
            "        print(exc)\n",
            "< with open(example.yaml()( ('yaml//example.py')))<EOS>\n",
            "\n",
            "> combine two lists `[1, 2, 3, 4]` and `['a', 'b', 'c', 'd']` into a dictionary\n",
            "= dict(zip([1, 2, 3, 4], ['a', 'b', 'c', 'd']))\n",
            "< dict(zip(list,1, 2, 3,,, (', ',',')])<EOS>\n",
            "\n",
            "> Add a tuple with value `another_choice` to a tuple `my_choices`\n",
            "= final_choices = ((another_choice,) + my_choices)\n",
            "< final_choices = ((choice(), + my_choices)<EOS>\n",
            "\n",
            "> Extract first two substrings in string `phrase` that end in `.`, `?` or `!`\n",
            "= re.match('(.*?[.?!](?:\\\\s+.*?[.?!]){0,1})', phrase).group(1)\n",
            "< re.split('[\\*\\(.((?\\.((])<EOS>\n",
            "\n",
            "> remove elements in list `b` from list `a`\n",
            "= [x for x in a if x not in b]\n",
            "< [x for x in a if x not in b]<EOS>\n",
            "\n",
            "> get set intersection between dictionaries `d1` and `d2`\n",
            "= dict((x, set(y) & set(d1.get(x, ()))) for x, y in d2.items())\n",
            "< dict((k, set(x) set& set( & set(d1.items()))<EOS>\n",
            "\n",
            "> pandas subtract a row from dataframe `df2` from dataframe `df`\n",
            "= pd.DataFrame(df.values - df2.values, columns=df.columns)\n",
            "< pd.DataFrame(df['1, // ',,,,'===))], axis=1)<EOS>\n",
            "\n",
            "> Match regex '[a-zA-Z][\\\\w-]*\\\\Z' on string 'A\\n'\n",
            "= re.match('[a-zA-Z][\\\\w-]*\\\\Z', 'A\\n')\n",
            "< re.sub('\\\\\\\\w\\\\\\w'|\\\\'\\ '\\\\')<EOS>\n",
            "\n",
            "> convert list with str into list with int\n",
            "= list(map(int, ['1', '2', '3']))\n",
            "< list(map(int, '[', 1))<EOS>\n",
            "\n",
            "> split string `s` into float values and write sum to `total`\n",
            "= total = sum(float(item) for item in s.split(','))\n",
            "< float(float(float(float(() for item in s'))( (',))))))<EOS>\n",
            "\n",
            "> Open file 'sample.json' in read mode with encoding of 'utf-8-sig'\n",
            "= json.load(codecs.open('sample.json', 'r', 'utf-8-sig'))\n",
            "< json..(codecs..('utf-8').(8')<EOS>\n",
            "\n",
            "> sort list `a` in ascending order based on the addition of the second and third elements of each tuple in it\n",
            "= sorted(a, key=lambda x: (sum(x[1:3]), x[0]))\n",
            "< sorted(a, key=lambda x:: (sum(x::sum::x [x)), x[0])<EOS>\n",
            "\n",
            "> unescape special characters without splitting data in array of strings `['I ', u'<', '3s U ', u'&', ' you luvz me']`\n",
            "= \"\"\"\"\"\".join(['I ', '<', '3s U ', '&', ' you luvz me'])\n",
            "< \"\"\"\"\"\"\"join(['I ', ', ','))<EOS>\n",
            "\n",
            "> save current figure to file 'graph.png' with resolution of 1000 dpi\n",
            "= plt.savefig('graph.png', dpi=1000)\n",
            "< plt.savefig('graph/png/png', dpi=')<EOS>\n",
            "\n",
            "> find the index of sub string 's' in string `str` starting from index 15\n",
            "= str.find('s', 15)\n",
            "< str.find(''. 15()<EOS>\n",
            "\n",
            "> find consecutive consonants in a word `CONCENTRATION` using regex\n",
            "= re.findall('[bcdfghjklmnpqrstvwxyz]+', 'CONCERTATION', re.IGNORECASE)\n",
            "< re.findall('[bcdfghjklmnpqrstvwxyz+\\''\\ '').split('\\']')<EOS>\n",
            "\n",
            "> Get all non-ascii characters in a unicode string `\\xa3100 is worth more than \\u20ac100`\n",
            "= print(re.sub('[\\x00-\\x7f]', '', '\\xa3100 is worth more than \\u20ac100'))\n",
            "< re[sub(sub([[\\[\\\\ \\\\ <]+\\\\\\, ', ' ', ' ', ').((')<EOS>\n",
            "\n",
            "> convert list of key-value tuples `[('A', 1), ('B', 2), ('C', 3)]` into dictionary\n",
            "= dict([('A', 1), ('B', 2), ('C', 3)])\n",
            "< dict([('(', 2, ', 2', ','), 2', ''), ',')<EOS>\n",
            "\n",
            "> check if list `seq` is empty\n",
            "= if (not seq):\n",
            "    pass\n",
            "< if (not seq)    pass    pass    pass    pass    pass:\n",
            "    pass<EOS>\n",
            "\n",
            "> replace only first occurence of string `TEST` from a string `longlongTESTstringTEST`\n",
            "= 'longlongTESTstringTEST'.replace('TEST', '?', 1)\n",
            "< 'longlongTESTstringTEST', 'TEST', 1,.(', '')<EOS>\n",
            "\n",
            "> Fit Kmeans function to a one-dimensional array `x` by reshaping it to be a multidimensional array of single values\n",
            "= km.fit(x.reshape(-1, 1))\n",
            "< km.fit(x(((1,),])<EOS>\n",
            "\n",
            "> Produce a string that is suitable as Unicode literal from string 'M\\\\N{AMPERSAND}M\\\\N{APOSTROPHE}s'\n",
            "= 'M\\\\N{AMPERSAND}M\\\\N{APOSTROPHE}s'.encode().decode('unicode-escape')\n",
            "< '\\\\N{AMPERSAND}'.\\\\N.'\\\\N')<EOS>\n",
            "\n",
            "> Check the status code of url `url`\n",
            "= r = requests.head(url)\n",
            "return (r.status_code == 200)\n",
            "< r = requests= requests.head(url)<EOS>\n",
            "\n",
            "> extract floating number from string 'Current Level: 13.4 db.'\n",
            "= re.findall('\\\\d+\\\\.\\\\d+', 'Current Level: 13.4 db.')\n",
            "< re.findall('\\\\\\d+\\\\.\\\\.4 db.\\\\\\$))<EOS>\n",
            "\n",
            "> immediately see output of print statement that doesn't end in a newline\n",
            "= sys.stdout.flush()\n",
            "< sys.stdout(flush(<EOS>\n",
            "\n",
            "> check if any of the items in  `search` appear in `string`\n",
            "= any(x in string for x in search)\n",
            "< any(x in string for x in search(<EOS>\n",
            "\n",
            "> convert a 1d `A` array to a 2d array `B`\n",
            "= B = np.reshape(A, (-1, 2))\n",
            "< B = np.reshape(A, (-1, 2)<EOS>\n",
            "\n",
            "> Get keys from a dictionary 'd' where the value is '1'.\n",
            "= print([key for key, value in list(d.items()) if value == 1])\n",
            "< [key for key[( value in list( value in list(d.items()))= (:10, 1))<EOS>\n",
            "\n",
            "> Get pandas GroupBy object with sum over the rows with same column names within  dataframe `df`\n",
            "= df.groupby(df.columns, axis=1).sum()\n",
            "< df.groupby(columns,columns,columns=)))( axis=)))<EOS>\n",
            "\n",
            "> return dataframe `df` with last row dropped\n",
            "= df.ix[:-1]\n",
            "< df.ix(-1:-[1]: np.mean)}<EOS>\n",
            "\n",
            "> count the number of items in a generator/iterator `it`\n",
            "= sum(1 for i in it)\n",
            "< random.((<EOS>\n",
            "\n",
            "> get a list of the row names from index of a pandas data frame\n",
            "= list(df.index)\n",
            "< df.index()<EOS>\n",
            "\n",
            "> plot point marker '.' on series `ts`\n",
            "= ts.plot(marker='.')\n",
            "< ts.plot(marker.'()<EOS>\n",
            "\n",
            "> convert dictionary `dict` into a flat list\n",
            "= print([y for x in list(dict.items()) for y in x])\n",
            "< print([y for x in list(dict.items()))<EOS>\n",
            "\n",
            "> for a dictionary `a`, set default value for key `somekey` as list and append value `bob`  in that key\n",
            "= a.setdefault('somekey', []).append('bob')\n",
            "< keys.setdefault('somekey.index(')( ='bob'))<EOS>\n",
            "\n",
            "> create a file 'filename' with each tuple in the list `mylist` written to a line\n",
            "= open('filename', 'w').write('\\n'.join('%s %s' % x for x in mylist))\n",
            "< open(',n', '%n')<EOS>\n",
            "\n",
            "> check if any element of list `substring_list` are in string `string`\n",
            "= any(substring in string for substring in substring_list)\n",
            "< any(substring in string for substring in substring(list))<EOS>\n",
            "\n",
            "> remove white space padding around a saved image `test.png` in matplotlib\n",
            "= plt.savefig('test.png', bbox_inches='tight')\n",
            "< plt.savefig_'test.png', bbox=<EOS>\n",
            "\n",
            "> generate unique equal hash for equal dictionaries `a` and `b`\n",
            "= hash(pformat(a)) == hash(pformat(b))\n",
            "< {{a_ ( ({:'}<EOS>\n",
            "\n",
            "> get a utf-8 string literal representation of byte string `x`\n",
            "= \"\"\"x = {}\"\"\".format(x.decode('utf8')).encode('utf8')\n",
            "< \"\"\"\"\"\"\".((-\"\"'.(((}'-\"\"\".('.-.('utf8'))<EOS>\n",
            "\n",
            "> Generate random integers between 0 and 9\n",
            "= print((random.randint(0, 9)))\n",
            "< print(random.randint(0 9))<EOS>\n",
            "\n",
            "> Remove characters in `b` from a string `a`\n",
            "= a = a.replace(char, '')\n",
            "< a . a.char(char( a)<EOS>\n",
            "\n",
            "> format current date to pattern '{%Y-%m-%d %H:%M:%S}'\n",
            "= time.strftime('{%Y-%m-%d %H:%M:%S}')\n",
            "< time.strftime('-{'%: '}-%m':%d :%<EOS>\n",
            "\n",
            "> Add row `['8/19/2014', 'Jun', 'Fly', '98765']` to dataframe `df`\n",
            "= df.loc[len(df)] = ['8/19/2014', 'Jun', 'Fly', '98765']\n",
            "< df.loc[loc[df[loc[df[loc='19/19.ext', 'Jun/19/19.ext')]<EOS>\n",
            "\n",
            "> make a function `f` that calculates the sum of two integer variables `x` and `y`\n",
            "= f = lambda x, y: x + y\n",
            "< f = lambda x<EOS>\n",
            "\n",
            "> get the ASCII value of a character 'a' as an int\n",
            "= ord('a')\n",
            "< ord(a, b)<EOS>\n",
            "\n",
            "> change legend font size with matplotlib.pyplot to 6\n",
            "= plot.legend(loc=2, prop={'size': 6})\n",
            "< plot.legend([=2, prop=', 'size')]<EOS>\n",
            "\n",
            "> group a list `list_of_tuples` of tuples by values\n",
            "= zip(*list_of_tuples)\n",
            "< zip(list(of_tuples)<EOS>\n",
            "\n",
            "> lowercase keys and values in dictionary `{'My Key': 'My Value'}`\n",
            "= {k.lower(): v.lower() for k, v in list({'My Key': 'My Value'}.items())}\n",
            "< {k.lower(::::::::k::::'v) for k, {}'.}'}<EOS>\n",
            "\n",
            "> get a list each value `i` in the implicit tuple `range(3)`\n",
            "= list(i for i in range(3))\n",
            "< list(i for i in range(chain())<EOS>\n",
            "\n",
            "> slice list `[1, 2, 3, 4, 5, 6, 7]` into lists of two elements each\n",
            "= list(grouper(2, [1, 2, 3, 4, 5, 6, 7]))\n",
            "< list(grouper(grouper( [1, 2, 3, 4, 6]))<EOS>\n",
            "\n",
            "> extract all rows from dataframe `data` where the value of column 'Value' is True\n",
            "= data[data['Value'] == True]\n",
            "< [[['Value', 'b'],=========)<EOS>\n",
            "\n",
            "> clear the terminal screen in Linux\n",
            "= os.system('clear')\n",
            "< os.system('clear')<EOS>\n",
            "\n",
            "> get an element at index `[1,1]`in a numpy array `arr`\n",
            "= print(arr[1, 1])\n",
            "< arr[0]0]1]''.(('2,)]<EOS>\n",
            "\n",
            "> get list of keys in dictionary `my_dict` whose values contain values from list `lst`\n",
            "= [key for item in lst for key, value in list(my_dict.items()) if item in value]\n",
            "< [key for key[ value in list[ value in list(my_.items()))<EOS>\n",
            "\n",
            "> specify multiple positional arguments with argparse\n",
            "= parser.add_argument('input', nargs='+')\n",
            "< parser_argument(input_argument,', nargs'))<EOS>\n",
            "\n",
            "> check if string `some_string` is empty\n",
            "= if (not some_string):\n",
            "    pass\n",
            "< if (not some(not some:<EOS>\n",
            "\n",
            "> find all `owl:Class` tags by parsing xml with namespace\n",
            "= root.findall('{http://www.w3.org/2002/07/owl#}Class')\n",
            "< {{..({}{(/)//://<EOS>\n",
            "\n",
            "> Convert a string `s` containing hex bytes to a hex string\n",
            "= s.decode('hex')\n",
            "< s.decode('hex')<EOS>\n",
            "\n",
            "> trim string \" Hello \"\n",
            "= ' Hello '.strip()\n",
            "< ' Hello '.strip()<EOS>\n",
            "\n",
            "> read file 'myfile' using encoding 'iso-8859-1'\n",
            "= codecs.open('myfile', 'r', 'iso-8859-1').read()\n",
            "< codecs.open('myfile', 'iso-8859-8859-8859-8859', ''--.-<EOS>\n",
            "\n",
            "> check if string `b` is a number\n",
            "= b.isdigit()\n",
            "< isdigit(isdigit(isdigit(isdigit(isdigit)<EOS>\n",
            "\n",
            "> SQLAlchemy select records of columns of table `my_table` in addition to current date column\n",
            "= print(select([my_table, func.current_date()]).execute())\n",
            "< print(select.table(table.table(  ( ..} ( .}()(<EOS>\n",
            "\n",
            "> Reverse a string \"foo\"\n",
            "= 'foo'[::(-1)]\n",
            "< 'foo':-1::::::::::\n",
            ":::')<EOS>\n",
            "\n",
            "> get current time\n",
            "= datetime.datetime.time(datetime.datetime.now())\n",
            "< time.datetime.now()<EOS>\n",
            "\n",
            "> write pandas dataframe `df` to the file 'c:\\\\data\\\\t.csv' without row names\n",
            "= df.to_csv('c:\\\\data\\\\t.csv', index=False)\n",
            "< df.to_csv('c', 'c',t'.read.sum',t=')<EOS>\n",
            "\n",
            "> close the window in tkinter\n",
            "= self.root.destroy()\n",
            "< self.root.destroy('<EOS>\n",
            "\n",
            "> get top `3` items from a dictionary `mydict` with largest sum of values\n",
            "= heapq.nlargest(3, iter(mydict.items()), key=lambda tup: sum(tup[1]))\n",
            "< heapq.nlargest(mydict.items().items())==lambda tup()) sum=lambda tup.items()))<EOS>\n",
            "\n",
            "> access an arbitrary value from dictionary `dict`\n",
            "= next(iter(list(dict.values())))\n",
            "< next.iter(dict.values())<EOS>\n",
            "\n",
            "> using beautifulsoup to select div blocks within html `soup`\n",
            "= soup.find_all('div', class_='crBlock ')\n",
            "< soup.find_all['div', class='_')<EOS>\n",
            "\n",
            "> write a tuple of tuples `A` to a csv file using python\n",
            "= writer.writerow(A)\n",
            "< writer.writerow(A,.-=*),,,=='),,==')<EOS>\n",
            "\n",
            "> truncate string `s` up to character ':'\n",
            "= s.split(':', 1)[1]\n",
            "< s[:split(split('\\': 1])<EOS>\n",
            "\n",
            "> variable number of digits `digits` in variable `value` in format string \"{0:.{1}%}\"\n",
            "= \"\"\"{0:.{1}%}\"\"\".format(value, digits)\n",
            "< \"\"\"{0{1{{{{0{0:{1}{}\"\".format(value)<EOS>\n",
            "\n",
            "> create new column `A_perc` in dataframe `df` with row values equal to the value in column `A` divided by the value in column `sum`\n",
            "= df['A_perc'] = df['A'] / df['sum']\n",
            "< df['perc']df['''] ''')= df df['value']]<EOS>\n",
            "\n",
            "> remove item `c` in list `a`\n",
            "= a.remove(c)\n",
            "< a.remove(c=c/c=<EOS>\n",
            "\n",
            "> get a new string from the 3rd character to the end of the string `x`\n",
            "= x[2:]\n",
            "< x[2222222222222222222222]<EOS>\n",
            "\n",
            "> get a dictionary with keys from one list `keys` and values from other list `data`\n",
            "= dict(zip(keys, zip(*data)))\n",
            "< dict(zip( zip(keys), zip())<EOS>\n",
            "\n",
            "> find element `a` that contains string \"TEXT A\" in file `root`\n",
            "= e = root.xpath('.//a[contains(text(),\"TEXT A\")]')\n",
            "< e = root.xpath('.//a._txt_test(\"\"\"\"TEXT A=)<EOS>\n",
            "\n",
            "> read json `elevations` to pandas dataframe `df`\n",
            "= pd.read_json(elevations)\n",
            "< json=elevations=elevations=elevations=elevations=elevations=json.get='=')<EOS>\n",
            "\n",
            "> Get the sum of values to the power of their indices in a list `l`\n",
            "= sum(j ** i for i, j in enumerate(l, 1))\n",
            "< sum(j * i for i)<EOS>\n",
            "\n",
            "> encode string \"\\\\xc3\\\\x85\" to bytes\n",
            "= \"\"\"\\\\xc3\\\\x85\"\"\".encode('utf-8')\n",
            "< \"\"\"\\\\xc3\\\\\"\".encode('utf-8')<EOS>\n",
            "\n",
            "> parse string `a` to float\n",
            "= float(a)\n",
            "< float(a[float(a)<EOS>\n",
            "\n",
            "> Get a list of all values in column `a` in pandas data frame `df`\n",
            "= df['a'].tolist()\n",
            "< df[~df['].tolist(tolist(values.values()[']')<EOS>\n",
            "\n",
            "> remove the element in list `a` at index `index`\n",
            "= del a[index]\n",
            "< aaaindex(True)<EOS>\n",
            "\n",
            "> get current url in selenium webdriver `browser`\n",
            "= print(browser.current_url)\n",
            "< browser.current_'url_<EOS>\n",
            "\n",
            "> multiply the columns of sparse matrix `m` by array `a` then multiply the rows of the resulting matrix by array `a`\n",
            "= numpy.dot(numpy.dot(a, m), a)\n",
            "< numpy.dot_dot,numpy.dot( m, a, a[1, ]) > 5, 7)<EOS>\n",
            "\n",
            "> Group a pandas data frame by monthly frequenct `M` using groupby\n",
            "= df.groupby(pd.TimeGrouper(freq='M'))\n",
            "< df.groupby('freq.TimeGrouper('=').=='='=')<EOS>\n",
            "\n",
            "> convert a list of lists `a` into list of tuples of appropriate elements form nested lists\n",
            "= zip(*a)\n",
            "< zip(*a)<EOS>\n",
            "\n",
            "> decode url `url` with utf8 and print it\n",
            "= print(urllib.parse.unquote(url).decode('utf8'))\n",
            "< print(urllib.parse.unquote(url) -=''))<EOS>\n",
            "\n",
            "> throw a ValueError with message 'represents a hidden bug, do not catch this'\n",
            "= raise ValueError('represents a hidden bug, do not catch this')\n",
            "< raise ValueError('represents a hidden bug' do not catch this:'<EOS>\n",
            "\n",
            "Correct on 101 snippets out of 2300 on training set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1bk8NbnYcsP",
        "colab_type": "code",
        "outputId": "3014c5ca-dfbf-48dc-e10f-6405dd08248c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#test set eval\n",
        "snippets=[w[1] for w in test_pairs]\n",
        "answers=evaluate_data_set(encoder1, attn_decoder1, test_pairs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> send a signal `signal.SIGUSR1` to the current process\n",
            "= os.kill(os.getpid(), signal.SIGUSR1)\n",
            "< your.communicate()<EOS>\n",
            "\n",
            "> convert a string `123,456.908` with dot and comma into a floating number\n",
            "= float('123,456.908'.replace(',', ''))\n",
            "< p ('(,, struct.csv',<EOS>\n",
            "\n",
            "> sum all elements of nested list `L`\n",
            "= sum(sum(i) if isinstance(i, list) else i for i in L)\n",
            "< print(sum(,,)),))<EOS>\n",
            "\n",
            "> Insert a 'None' value into a SQLite3 table.\n",
            "= db.execute(\"INSERT INTO present VALUES('test2', ?, 10)\", (None,))\n",
            "< ((((()())(<EOS>\n",
            "\n",
            "> format string \"({0.goals} goals, ${0.penalties})\"\n",
            "= \"\"\"({0.goals} goals, ${0.penalties})\"\"\".format(self)\n",
            "< {{s.'}()(://<EOS>\n",
            "\n",
            "> create list `lst` containing 100 instances of object `Object`\n",
            "= lst = [Object() for i in range(100)]\n",
            "< ((((1,10)((10))<EOS>\n",
            "\n",
            "> count the occurrences of item \"b\" in list `l`\n",
            "= l.count('b')\n",
            "< sum(l)<EOS>\n",
            "\n",
            "> change working directory to the directory `owd`\n",
            "= os.chdir(owd)\n",
            "< os.path.realpath('(path/\\\\))\\)\\)\\)()()()()())())<EOS>\n",
            "\n",
            "> extract all the values with keys 'x' and 'y' from a list of dictionaries `d` to list of tuples\n",
            "= [(x['x'], x['y']) for x in d]\n",
            "< [(x k, v in d) for k, v in d.items()]<EOS>\n",
            "\n",
            "> shuffle columns of an numpy array 'r'\n",
            "= np.random.shuffle(np.transpose(r))\n",
            "< np.all(axis( axis=))0, axis=0)0)0)0)0)<EOS>\n",
            "\n",
            "> Removing duplicates in list `abracadabra`\n",
            "= list(OrderedDict.fromkeys('abracadabra'))\n",
            "< ((t)1, 1)<EOS>\n",
            "\n",
            "> Get attribute `my_str` of object `my_object`\n",
            "= getattr(my_object, my_str)\n",
            "< ((())<EOS>\n",
            "\n",
            "> Check if any key in the dictionary `dict1` starts with the string `EMP$$`\n",
            "= any(key.startswith('EMP$$') for key in dict1)\n",
            "< {(\".}'.((}(}(.}<EOS>\n",
            "\n",
            "> get the position of item 1 in `testlist`\n",
            "= for i in [i for (i, x) in enumerate(testlist) if (x == 1)]:\n",
            "    pass\n",
            "< [x[1:2]['((()2])<EOS>\n",
            "\n",
            "> unzip list `original` and return a generator\n",
            "= result = ((a for (a, b) in original), (b for (a, b) in original))\n",
            "< (((((())((()<EOS>\n",
            "\n",
            "> Django response with JSON `data`\n",
            "= return HttpResponse(data, mimetype='application/json')\n",
            "< data.find(['data', '))<EOS>\n",
            "\n",
            "> append list `list1` to `list2`\n",
            "= list2.extend(list1)\n",
            "< raise Exception('[\\', ').append('\\')]<EOS>\n",
            "\n",
            "> place the radial ticks in plot `ax` at 135 degrees\n",
            "= ax.set_rlabel_position(135)\n",
            "< (..(((__)([1], ', x')<EOS>\n",
            "\n",
            "> create a new column `weekday` in pandas data frame `data` based on the values in column `my_dt`\n",
            "= data['weekday'] = data['my_dt'].apply(lambda x: x.weekday())\n",
            "< df.groupby([',B',')], axis=))<EOS>\n",
            "\n",
            "> declare an array\n",
            "= my_list = []\n",
            "< numpy.array(your( ensure, dtype=False)<EOS>\n",
            "\n",
            "> delete all characters \"i\" in string \"it is icy\"\n",
            "= \"\"\"it is icy\"\"\".replace('i', '')\n",
            "< ((((2(((*[1])<EOS>\n",
            "\n",
            "> get an array of the mean of each two consecutive values in numpy array `x`\n",
            "= x[:-1] + (x[1:] - x[:-1]) / 2\n",
            "< np.where(x[1] for row in x(])<EOS>\n",
            "\n",
            "> concatenate array of strings `['A', 'B', 'C', 'D']` into a string\n",
            "= \"\"\"\"\"\".join(['A', 'B', 'C', 'D'])\n",
            "< \"\"\"\"\"\".join((('((( ')(),'}<EOS>\n",
            "\n",
            "> Django get maximum value associated with field 'added' in model `AuthorizedEmail`\n",
            "= AuthorizedEmail.objects.filter(group=group).order_by('-added')[0]\n",
            "< User.(((((_(/[1)=2,')<EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owWaTxozmmEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_snippets=[[c for c in s] for s in snippets]\n",
        "c_answers=[[c for c in s] for s in answers]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18_iAyLGWF2E",
        "colab_type": "code",
        "outputId": "971fe2a1-2cb3-45af-b893-d8dcaa8f15e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "blue_nltk=sentence_bleu\n",
        "cc=SmoothingFunction()\n",
        "score=0\n",
        "acc=0\n",
        "for i in range(len(c_answers)):\n",
        "  score+=blue_nltk([c_snippets[i]], c_answers[i],smoothing_function=cc.method4)\n",
        "  if c_snippets[i]==c_answers[i]:\n",
        "    acc+=1\n",
        " \n",
        "print(\"Avg Bleu scroe is: {}\".format(score/len(c_answers)))\n",
        "print(\"Correct on {} snippets out of {} on test set\".format(acc, len(c_answers)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg Bleu scroe is: 0.12650858422796452\n",
            "Correct on 0 snippets out of 477 on test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgEBmjPKj1bG",
        "colab_type": "code",
        "outputId": "522928b8-78ab-4138-e8ab-d845d8c6d9ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "ls = [[\"check if item from list `b` is in list `a`\",\"print(x in a for x in b)\"]]\n",
        "n_lang = evaluate_data_set(encoder1,attn_decoder1, ls)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> check if item from list `b` is in list `a`\n",
            "= print(x in a for x in b)\n",
            "< any(any(x in a for x in b) for x in a(<EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJWZORUzkJRB",
        "colab_type": "code",
        "outputId": "7c2b2dbc-6b60-4645-d268-f307b4cf5fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "ls = [[\"create a matrix from a list `[4, 5, 6]\",\"x = scipy.matrix([4, 5, 6]).transpose()\"]]\n",
        "n_lang = evaluate_data_set(encoder1,attn_decoder1, ls)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> create a matrix from a list `[4, 5, 6]\n",
            "= x = scipy.matrix([4, 5, 6]).transpose()\n",
            "< [(x.max(x.max(),,])<EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}